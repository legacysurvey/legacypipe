import fitsio
import numpy as np
from glob import glob
import os
from legacypipe.survey import LegacySurveyData

'''

This script was used (after creating a symlink farm as described
below) to merge in an updated set of bricks in *sourcedir* into the
symlink farm in *destdir*.  Specifically, the *sourcedir* was re-run
bricks that fixed a sub-blobs issue in DR10; the *destdir* was the new
DR10.1 directory (symlink farm).

It does, for each brick:

- in the source directory, check the checksum file
- look for the known set of output files
- for per-band files, if one file per band is found, demand that all files are found
  (ie, files for a band either all exist or none do)
- assert that all files are in the checksums file
- update the checksums files, merging old & new
- delete target directory 'coadd/B/B' if it is a symlink
- delete any other target files that are symlinks
- rsync files into place
- write updated checksums files

Symlink farm was generated by first creating lists of the 3-character
RA slices (000, 001, ...) that contained no new files, in "sym.txt",
and those that do contain new files in "sb.txt".  The total number of
lines in the two files should be 360.

### Whole RA slices with no updated bricks:

cd ~/cosmo/work/legacysurvey/dr10.1/south

for x in $(cat sym.txt); do ln -s /global/cfs/cdirs/cosmo/data/legacysurvey/dr10/south/coadd/$x coadd/; done

for x in $(cat sym.txt); do ln -s /global/cfs/cdirs/cosmo/data/legacysurvey/dr10/south/metrics/$x metrics/; done

for x in $(cat sym.txt); do ln -s /global/cfs/cdirs/cosmo/data/legacysurvey/dr10/south/tractor/$x tractor/; done

for x in $(cat sym.txt); do ln -s /global/cfs/cdirs/cosmo/data/legacysurvey/dr10/south/tractor-i/$x tractor-i/; done

### RA slices with updated bricks:

for x in $(cat sb.txt); do echo $x; mkdir coadd/$x;     for y in /global/cfs/cdirs/cosmo/data/legacysurvey/dr10/south/coadd/$x/*; do ln -s $y coadd/$x/; done; done

for x in $(cat sb.txt); do echo $x; mkdir metrics/$x;   for y in /global/cfs/cdirs/cosmo/data/legacysurvey/dr10/south/metrics/$x/*; do ln -s $y metrics/$x/; done; done

for x in $(cat sb.txt); do echo $x; mkdir tractor/$x;   for y in /global/cfs/cdirs/cosmo/data/legacysurvey/dr10/south/tractor/$x/*; do ln -s $y tractor/$x/; done; done

for x in $(cat sb.txt); do echo $x; mkdir tractor-i/$x; for y in /global/cfs/cdirs/cosmo/data/legacysurvey/dr10/south/tractor-i/$x/*; do ln -s $y tractor-i/$x/; done; done

'''

def main():

    sourcedir = '/pscratch/sd/d/dstn/sub-blobs'
    destdir = '/global/cfs/cdirs/cosmo/work/legacysurvey/dr10.1/south'

    # Get list of checksum files (one per brick), from which we will get
    # the list of bricks to update.
    fns = glob(os.path.join(sourcedir, 'tractor/*/brick-*.sha256sum'))
    fns.sort()
    survey = LegacySurveyData(survey_dir=sourcedir)

    # Optical and infrared bands to include
    bands = ['g', 'r', 'i', 'z']
    wbands = ['W1', 'W2', 'W3', 'W4']

    for fn in fns:
        print()
        # Assume 8-character brick names
        brick = fn.replace('.sha256sum', '')[-8:]
        print('Brick', brick)
        # Pull off the base directory (which will be the same as sourcedir)
        basedir = '/'.join(fn.split('/')[:-3])
        # Check the source-directory checksum
        cmd = 'cd %s && sha256sum --quiet -c %s' % (basedir, fn)
        print(cmd)
        rtn = os.system(cmd)
        assert(rtn == 0)
        # Collect the filenames listed in the source checksum file
        shas = open(fn).readlines()
        shas = set([s.strip().split()[1].replace('*','') for s in shas])

        # Check that all expected filetypes (that aren't per-band) exist for this brick
        allfns = []
        for filetype in ['tractor', 'tractor-intermediate', 'ccds-table', 'depth-table',
                         'image-jpeg', 'model-jpeg', 'resid-jpeg',
                         'blobmodel-jpeg',
                         'wise-jpeg', 'wisemodel-jpeg', 'wiseresid-jpeg',
                         'outliers-pre', 'outliers-post',
                         'outliers-masked-pos', 'outliers-masked-neg',
                         'outliers_mask',
                         'blobmap', 'maskbits', 'all-models', 'ref-sources',
                         ]:
            fn = survey.find_file(filetype, brick=brick)
            assert(os.path.exists(fn))
            # Record the relative path
            allfns.append(fn.replace(basedir+'/', ''))

        # Check that, for each optical band, either all the expected files exist,
        # or none do.
        for band in bands:
            for i,filetype in enumerate(['invvar', 'chi2', 'image', 'model', 'blobmodel',
                                         'depth', 'galdepth', 'nexp', 'psfsize',]):
                fn = survey.find_file(filetype, brick=brick, band=band)
                exists = os.path.exists(fn)
                # Either all products exist for a band, or none!
                if i == 0:
                    has_band = exists
                else:
                    assert(has_band == exists)
                if has_band:
                    # Record the relative path
                    allfns.append(fn.replace(basedir+'/', ''))
            print('Band', band, 'exists:', has_band)

        # Check that all expected WISE files exist
        for band in wbands:
            for i,filetype in enumerate(['invvar', 'image', 'model']):
                fn = survey.find_file(filetype, brick=brick, band=band)
                assert(os.path.exists(fn))
                # Record the relative path
                allfns.append(fn.replace(basedir+'/', ''))

        # Check that the set of files found equals the set listed in the checksum file.
        assert(set(shas) == set(allfns))

        # Collect the source checksums (filename -> checksum map)
        new_checksums = {}
        fn = survey.find_file('checksums', brick=brick)
        for line in open(fn).readlines():
            words = line.split()
            fn = words[1]
            if fn.startswith('*'):
                fn = fn[1:]
            assert(fn in allfns)
            new_checksums[fn] = words[0]

        # Create updated (merged) checksum files
        new_checksum_files = []
        # Find the set of directories that source files will go into
        alldirs = set([os.path.dirname(x) for x in allfns])
        for dirnm in alldirs:
            # Find the existing checksum file in the destination directory
            pat = os.path.join(destdir, dirnm, '*.sha256sum')
            sha = glob(pat)
            assert(len(sha) == 1)
            sha = sha[0]

            print('Updating existing checksum file:', sha)
            # Read destination checksum file (map filename -> checksum)
            checksums = {}
            for line in open(sha).readlines():
                words = line.split()
                fn = words[1]
                if fn.startswith('*'):
                    fn = fn[1:]
                checksums[fn] = words[0]

            # Update destination checksums with source checksums
            nup = 0
            for fn in allfns:
                if not fn.startswith(dirnm):
                    continue
                assert(fn in new_checksums)
                base = os.path.basename(fn)
                assert(base in checksums)
                checksums[base] = new_checksums[fn]
                nup += 1
            print('Updated checksum for', nup, 'files')

            # Record the updated checksum contents and filename, for later writing.
            chk_txt = ''.join(['%s *%s\n' % (v,k) for k,v in checksums.items()])
            new_checksum_files.append((sha, chk_txt))

        # Delete existing destination directories (for the coadd dir only),
        # but only if they are symlinks
        # (useful when updating some bricks into a symlink farm)
        for dirnm in ['coadd/%s/%s' % (brick[:3], brick)]:
            path = os.path.join(destdir, dirnm)
            if os.path.islink(path):
                print('Deleting symlink', path)
                os.remove(path)

        # Delete existing destination files, but only if they are symlinks
        for fn in allfns:
            path = os.path.join(destdir, fn)
            if os.path.islink(path):
                print('Deleting symlink', path)
                os.remove(path)

        # Copy files into place.
        cmd = 'rsync -Rarv %s/./{%s} %s' % (basedir, ','.join(allfns), destdir)
        print('Rsyncing: rsync -Rarv %s/./{[files]} %s' % (basedir, destdir))
        rtn = os.system(cmd)
        assert(rtn == 0)

        # Write updated checksum files.
        for path, chksum in new_checksum_files:
            if os.path.islink(path):
                print('Deleting symlink', path)
            print('writing checksum', path)
            with open(path + '.new', 'w') as f:
                f.write(chksum)
            os.rename(path + '.new', path)

            # Check new checksums -- expensive for, eg, tractor/001 !
            # dirnm = os.path.dirname(path)
            # cmd = 'cd %s && sha256sum --quiet -c %s' % (dirnm, os.path.basename(path))
            # print(cmd)
            # rtn = os.system(cmd)
            # assert(rtn == 0)

if __name__ == '__main__':
    main()
